{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM アルゴリズム\n",
    "\n",
    "### KL ダイバージェンスと最尤推定の関係\n",
    "\n",
    "> - 入力\n",
    ">   - 真の確率分布 $p_\\ast (x)$ から生成された $N$ 個のサンプル $\\{x^{(1)},x^{(2)},\\ldots,x^{(N)}\\}$\n",
    ">   - パラメータ $\\theta$ で調整できる確率分布 $p_\\theta(x)$\n",
    "> - 出力\n",
    ">   - $p_\\theta(x)$ が $p_\\ast(x)$ に最も近づくような $\\theta$ の値\n",
    "\n",
    "最尤推定では，以下の対数尤度を目的関数とする．\n",
    "\n",
    "$$\n",
    "\\log \\prod_{n = 1}^N p_\\theta (x^{(n)}) = \\sum_{n = 1}^N \\log p_\\theta(x^{(n)})\n",
    "$$\n",
    "\n",
    "この対数尤度を最大化するパラメータを以下のように表記する．\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\underset{\\theta}{\\mathrm{argmax}} \\sum_{n = 1}^N \\log p_\\theta(x^{(n)})\n",
    "$$\n",
    "\n",
    "ここで，「$p_\\theta(x)$ を $p_\\ast(x)$ に近づける」は，\n",
    "「$p_\\theta(x)$ と $p_\\ast(x)$ の KL ダイバージェンスを最小にする」と言い換えられる．\n",
    "\n",
    "すなわち，\n",
    "\n",
    "$$\n",
    "D_\\mathrm{KL}(p_\\ast || p_\\theta) = \\int p_\\ast(x) \\log \\frac{p_\\ast(x)}{p_\\theta(x)} dx\n",
    "$$\n",
    "\n",
    "を最小にするような $\\theta$ を求められればよい．\n",
    "\n",
    "しかし，$p_\\ast$ が未知である以上，実際に積分計算を行うことはできない…\n",
    "\n",
    "そこで，**モンテカルロ法**を用いて近似的に求めることを考える．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モンテカルロ法を用いた期待値の近似\n",
    "\n",
    "確率分布 $p(x)$ に対する任意の関数 $f(x)$ の期待値 $\\mathbb{E}_{p(x)}[f(x)]$ を以下のように定義する．\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{p(x)}[f(x)] = \\int p(x)f(x) dx\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "name": "rust"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
