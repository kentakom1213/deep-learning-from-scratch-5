## EM アルゴリズム

### EM アルゴリズムと最尤推定の関係

> - 入力
>   - 真の確率分布 $p_\ast (x)$ から生成された $N$ 個のサンプル $\{x^{(1)},x^{(2)},\ldots,x^{(N)}\}$
>   - パラメータ $\theta$ で調整できる確率分布 $p_\theta(x)$
> - 出力
>   - $p_\theta(x)$ が $p_\ast(x)$ に最も近づくような $\theta$ の値

最尤推定では，以下の対数尤度を目的関数とする．

$$
\log \prod_{n = 1}^N p_\theta (x^{(n)}) = \sum_{n = 1}^N \log p_\theta(x^{(n)})
$$

この対数尤度を最大化するパラメータを以下のように表記する．

$$
\hat{\theta} = \underset{\theta}{\mathrm{argmax}} \sum_{n = 1}^N \log p_\theta(x^{(n)})
$$

ここで，「$p_\theta(x)$ を $p_\ast(x)$ に近づける」は，
「$p_\theta(x)$ と $p_\ast(x)$ の KL ダイバージェンスを最小にする」と言い換えられる．

すなわち，

$$
D_\mathrm{KL}(p_\ast || p_\theta) = \int p_\ast(x) \log \frac{p_\ast(x)}{p_\theta(x)} dx
$$

を最小にするような $\theta$ を求められればよい．

しかし，$p_\ast$ が未知である以上，実際に積分計算を行うことはできない…

そこで，**モンテカルロ法**を用いて近似的に求めることを考える．

---

EM アルゴリズムは，GMM だけでなく「潜在変数」をもつモデル一般に適用できるアルゴリズムである．

### 潜在変数を持つモデル

潜在変数を持つモデルは，

- 観測可能な確率変数: $x$
- 潜在変数: $z$
- パラメータ: $\theta$

としたとき，

$$
\log p_\theta(x) = \log \sum_z p_\theta (x, z)
$$

と表される．

これは，「log-sum」の形を取っているため解析が難しい．

EM アルゴリズムでは，この問題を「sum-log」の形に変換して解く．
